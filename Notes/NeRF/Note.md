# NeRF
**论文链接** https://arxiv.org/abs/2003.08934  

## 内容概述
这篇论文中的方法主要做的事情是：  
对于一个3D场景  
1.从不同角度拍很多张照片并记录相机的位置和角度构建训练数据集  
2.基于训练数据集，训练一个多层mlp  
3.对于任意一个新输入的相机位置和角度，使用多层mlp来预测在新角度下，相机能拍摄到的2D图像  

Note：  
1.3D场景的信息是存储在训练之后的mlp参数中的。任意一次训练得到的一组mlp参数只能用于生成同一个3D场景的新视角照片  
2.mlp不直接预测整张图片，而是只预测空间中一个点，在特定观察方向上表现出来的颜色和遮光率

## 体素渲染算法原理
我们把3D场景看成空间中的一堆发光点，每个点在特定角度看会有一定的颜色和遮光率（因为它们互相遮蔽，靠近相机的点会遮住它身后点发出的光线）。  
体素渲染算法解决的问题是：假设我们已经知道了空间中每个点的在任意角度的颜色和遮光率（由训练的mlp给出），如何重建从特定位置特定角度看到的2D图像。 

Note：  
mlp是把输入（空间中某个点的坐标x，y，z，和看它的方向$\theta,\phi$）映射到输出（这个点在这个方向上的颜色r，g，b和遮光率$\sigma$）的**连续**函数。因此虽然说把3D场景看成很多点，其实这些点在空间中是稠密的，处处都是。只不过对应这空气的点从各个角度看遮光率都近似是0，对应不透光表面的点从特定角度看遮光率近似为1  

一条照到摄像机上的光线颜色可以用下面的公式表示： 
$$C(r)=\int_{t_n}^{t_f}T(t)\sigma(r(t))c(r(t),d)dt, T(t)=exp\{-\int_{t_n}^t\sigma(r(s))ds\}$$  
其中r是空间中的一条射线，表示为**o**+t**d**。由于光路是可逆的，一条由发光点发出相互遮蔽、叠加、最终进入摄影机的光线，可以看作一条摄影机发出的，被发光点吸收、直到最远处为0的光线。  
从这个角度来看，上面积分式中T(t)表示到t处剩下的光线，在离散情形下，它满足：  
$$T(t+\Delta t)=T(t)(1-\sigma(t)\Delta t)$$  
即经过$\Delta t$路程后$(1-\sigma(t)\Delta t)$的光线剩下了，在微分情形下求解微分方程即可得到T(t)的积分式。

在具体实现计算时，会把射线$t_n$和$t_f$之间的部分N等分，从每一小段中随机采样1个点，用求和近似积分，即有第i个点满足：  
$$t_i\sim \mathbf{u}[t_n+\frac{i-1}{N}(t_f-t_n),t_n+\frac{i}{N}(t_f-t_n)]$$  
对应C(r)的计算公式为：  
$$\hat{C}(r)=\sum T_i(1-exp(-\sigma_i\delta_i))C_i$$
$$T_j=exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j)$$
$\delta_i$是采样点对应的一小段线段的长度

### 算法中使用的trick
#### 1. Positional Encoding
虽然mlp理论上只有5个输入(x,y,z,$\theta$,$\phi$)，但是这样效果不好，所以采用Position Encoding的方法把输入映射到更高维度的空间再输入神经网络，具体是：  
$$\gamma (p)=(sin(2^0\pi p),cos(2^0\pi p),\ldots ,sin(2^{l-1}\pi p),cos(2^{l-1}\pi p))$$
对于**x**, l=10；对于**d**, l=4  

Note:  
笔者觉得一个直观的解释是，这样即可以捕获输入向量的微小变化，又可以捕获输入向量的巨大变化。从机器学习理论来看，相当于用了一个kernel把输入映射到高维，使得模型的表示能力增强了。 

#### 2.Hierachical volume sampling
这个没啥好说的，就是空间中有的点对三维重建不重要（例如空气），有的点比较重要（例如物体边界）。因此论文作者训练了两个神经网络，一个先等间距地采样，获得各个点的重要性权重，基于权重进行第二轮采样，使用精细的模型计算各个点的贡献，最后把两个模型的输出一起拿来做渲染。